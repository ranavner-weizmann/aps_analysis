{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05179fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import importlib.util\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../code/src/')\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33695f",
   "metadata": {},
   "source": [
    "#### CSV creation from stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27fcf31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avnerran/MSc/aps_analysis/transformers/notebooks/../code/src/utils.py:162: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  merged_df = numeric_df.groupby(numeric_df.columns, axis=1).mean()\n"
     ]
    }
   ],
   "source": [
    "# Shamat\n",
    "shamat_df = utils.get_df_from_shamat_csv('../data/raw/shamat/shamat_beit_dagan.csv')\n",
    "radiation_df = utils.get_df_from_radiation_csv('../data/raw/shamat/radiation_beit_dagan.csv', )\n",
    "shamat_df['rad'] = radiation_df['rad']\n",
    "\n",
    "if not os.path.exists('../data/processed_csvs/'):\n",
    "    os.makedirs('../data/processed_csvs/')\n",
    "shamat_df.to_csv('../data/processed_csvs/shamat.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "jerusalem_bikha.xlsx\n",
      "nir_galim.xlsx\n",
      "tel_aviv_yefet.xlsx\n",
      "bnei_atarot.xlsx\n",
      ".DS_Store\n",
      "karmei_yosef.xlsx\n",
      "yavne.xlsx\n",
      "ariel.xlsx\n",
      "tel_aviv_levinsky.xlsx\n",
      "modiin.xlsx\n",
      "ramle_omanim.xlsx\n",
      "holon.xlsx\n",
      "kfar_menachem_harhava.xlsx\n",
      "ahisemech.xlsx\n",
      "tel_aviv_lehi.xlsx\n",
      "gan_yavne.xlsx\n",
      "ashdod_tsfoni.xlsx\n",
      "elad.xlsx\n",
      "beit_shemesh.xlsx\n",
      "timorim.xlsx\n",
      "bnei_darom.xlsx\n",
      "beit_heshmonai.xlsx\n",
      "rehovot.xlsx\n",
      "kiryat_malachi.xlsx\n",
      "gan_darom.xlsx\n",
      "or_yehuda.xlsx\n",
      "kfar_menachem.xlsx\n",
      "yad_binyamin.xlsx\n",
      "hevel_yavne.xlsx\n",
      "yad_rambam.xlsx\n",
      "ashdod_kala.xlsx\n",
      "rishon_herzel.xlsx\n",
      "jerusalem_atarot.xlsx\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('../data/raw/ep/')))\n",
    "le = 0\n",
    "for i in os.listdir('../data/raw/ep/'):\n",
    "    print(i)\n",
    "    le += 1\n",
    "\n",
    "print(le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbd35072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of station: 32\n",
      "num of stations with factor0: 13\n",
      "num of stations without factor0: 19\n"
     ]
    }
   ],
   "source": [
    "# EP\n",
    "# shamat_csv = pd.read_csv('../data/processed_csvs/shamat.csv')\n",
    "stations_w_f0 = 0\n",
    "station_wo_f0 = 0\n",
    "\n",
    "for file in os.listdir('../data/raw/ep/'):\n",
    "    if file.endswith('.xlsx'):\n",
    "        station_df = utils.get_df_from_excel_station_file(f'../data/raw/ep/{file}')\n",
    "\n",
    "        # adding the factor0 to the csv:\n",
    "        try:\n",
    "            station_df['factor0'] = utils.calculate_factor0(station_df, shamat_df)\n",
    "            if station_df['factor0'].notna().any():\n",
    "                stations_w_f0 += 1\n",
    "            else:\n",
    "                station_wo_f0 += 1\n",
    "            station_df.to_csv(f'../data/processed_csvs/{file[:-5]}.csv', index=True)\n",
    "            \n",
    "        except:\n",
    "            print(f'probably no measurement for the factor calculation in station: {file[:-5]}')\n",
    "            station_df.to_csv(f'../data/processed_csvs/{file[:-5]}.csv', index=True)\n",
    "            \n",
    "            continue\n",
    "\n",
    "print(f'total num of station: {stations_w_f0 + station_wo_f0}')\n",
    "print(f'num of stations with factor0: {stations_w_f0}')\n",
    "print(f'num of stations without factor0: {station_wo_f0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8ae91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n the dates of the sessions are:\\n\\n4-5 May 2023\\n8-9 May 2023\\n23-24 May 2023\\n\\n25-26 August 2023\\n28-29 August 2023\\n '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " the dates of the sessions are:\n",
    "\n",
    "4-5 May 2023\n",
    "8-9 May 2023\n",
    "23-24 May 2023\n",
    "\n",
    "25-26 August 2023\n",
    "28-29 August 2023\n",
    " '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f670db",
   "metadata": {},
   "source": [
    "#### Pulling timestamps from pop tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5f33157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the time of the flights:\n",
    "# the time in the list is the time at the center of the flight\n",
    "flight_times = torch.load('../data/raw/drone/pom_timestamps.pt', map_location='cpu', weights_only=False)\n",
    "flight_time_list = flight_times.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b0fd3",
   "metadata": {},
   "source": [
    "#### Saving the stations as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ff4610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n",
      "torch.Size([66, 5, 15])\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../data/station_tensors/'):\n",
    "    os.makedirs('../data/station_tensors/')\n",
    "\n",
    "\n",
    "for station in os.listdir('../data/processed_csvs/'):\n",
    "    if station.endswith('.csv'):\n",
    "        t = utils.station_csv_to_tensor(f'../data/processed_csvs/{station}', flight_time_list, max_features=15)\n",
    "        print(t.shape)\n",
    "        torch.save(t, f'../data/station_tensors/{station[:-4]}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2595de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tensors shapes are [P, L, Fs]: [66, 5, 15] when the missing features are nan values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7abb7",
   "metadata": {},
   "source": [
    "#### Creating the metadata json files for each stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2492f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in os.listdir('../data/processed_csvs/'):\n",
    "    if station.endswith('.csv'):\n",
    "        station = station[:-4]\n",
    "        meta = utils.save_tensor_metadata(f\"../data/processed_csvs/{station}.csv\", station, tensor_shape=[[66, 5, 15]] output_dir='../data/metadata/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2926037",
   "metadata": {},
   "source": [
    "Now we have a tensor for each station: [P, L, Fs].  \n",
    "Plus a metadata json file for each station.  \n",
    "\n",
    "Plus, we have the tensor of the Ozone measurements: [P, h, Fd]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
